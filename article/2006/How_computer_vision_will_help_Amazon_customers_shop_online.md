# [How computer vision will help Amazon customers shop online](https://www.amazon.science/blog/how-computer-vision-will-help-amazon-customers-shop-online?utm_campaign=piqcy&utm_medium=email&utm_source=Revue%20newsletter)


[Image Search with Text Feedback by Visiolinguistic Attention Learning](https://assets.amazon.science/01/7b/af090cd94d609a6d9d5a75764e1d/scipub-1303.pdf), [Fashion Outfit Complementary Item Retrieval](https://assets.amazon.science/72/77/286db9ed405b859ba6cf161fec31/scipub-1282.pdf), [Image Based Virtual Try-on Network from Unpaired Data](https://assets.amazon.science/1a/2b/7a4dd8264ce19a959559da799aff/scipub-1281.pdf)の3つの論文を紹介

1. 言語による画像検索(もうちょい明るい色の服など、)
2. 補完的アイテム検索（このアウターに合わせるインナー）
3. 仮想試着（画像の生成）

## 課題

テキストを用いて製品クエリに一致する画像を探索する課題は、
1. テキストによる説明と画像の特徴を1つの表現に埋め込む
2. 1の埋め込むを様々な解像度で行うこと
3. ネットワークの訓練を通して、一部の画像特徴を維持しながら、指示に従って他の特徴を変更すること

# Image Search with Text Feedback by Visiolinguistic Attention Learning

## アブスト

>テキストフィードバックを用いた画像検索は、電子商取引やインターネット検索などの実世界での様々なアプリケーションで有望な効果を発揮します。参照画像とユーザからのテキストフィードバックが与えられると、入力画像を再構成するだけでなく、与えられたテキストと一致しない部分を変化させた画像を検索することが目標となります。これは、画像とテキストの両方を相乗的に理解する必要があるため、難しい課題である。 本研究では、この課題に、新たな視覚学習的注意学習(VAL)フレームワークを用いて取り組む。 具体的には、言語セマンティクスを条件とした視覚特徴を選択的に保存・変換するために、CNNにシームレスに接続可能な複合変換器を提案する。 複数の複合変換器を深さを変えて挿入することで、多階調の視覚情報をカプセル化し、効果的な画像検索のための表現力を得ることができる。 本研究では、3つのデータセットを用いて総合的な評価を行っている。 Fashion200k、Shoes、FashionIQの3つのデータセットについて総合的な評価を行った。広範な実験の結果、我々のモデルは全てのデータセットにおいて既存のアプローチを凌駕しており、属性的な記述や自然言語記述を含む様々なテキストフィードバックに対応する上で優位性があることを示した。

+ novel Visiolin-guistic Attention Learning (VAL) framework

やりたいこと-> Figure. 1
課題 -> 画像とテキストの両方の理解が必要
やったこと -> VALを導入


## Intor

 画像検索で最も普及しているパラダイムは、画像またはテキストのいずれかを入力クエリとして使用して、興味のある項目を検索するもので、一般的に画像対画像[15]やテキスト対画像マッチング[12]として知られています。   しかし、これらのパラダイムの本質的な欠点は、ユーザーの意図に合わせて検索項目を絞り込むことができないという点にある。

過去の提案手法

インタラクションのほとんどは、特定の属性[18, 79, 2]や相対的な属性[45, 28, 75]を記述したテキストの形で提供され、参照画像を改良したり修正している。実際には大きな潜在的価値を持っているにもかかわらず、画像検索に様々なタイプのテキストフィードバックを組み込むことはまだ研究されていない。

=> 結局、テキスト形式の属性を付与しており、ざまざまな検索に対応できないじゃん。

#### この研究では????

本研究では、ユーザが参照画像を選択し、テキストを追加して検索結果を修正することで、システムとの対話を可能にするテキストフィードバックを用いた画像検索の研究を行っています。

これまでの研究では、テキストのフィードバックに焦点を当てていましたが、本研究では、属性的な記述や自然言語表現など、より一般的なテキストの形態を考慮しています。

他の研究と異なる点?

>Thisis because, it uniquely entails learning a composite repre-sentation that can jointly capture visual cues and linguisticinformation to match the target image of interest.
その理由は、視覚的手がかりと言語情報を共同で捕捉し、関心のある対象画像と一致させることができる複合的な再提示を学習することに特徴があるからである。

=> そのためには、テキストとの調和を図りながら、視覚的なコンテンツを同時に保存・変換することの難しさが課題となる。 例えば、テキストが修正する色を指定した場合（図1(a)）、シルエット、パターン、トリムなどの他の視覚的な手がかりはすべて保存され、色だけが目的の色に変換されている必要があることを意味しています。  また、テキストフィードバックは多段階の意味論（曖昧）を伝える可能性があるため（図1）、複合表現は多階調の視覚言語情報を捉えることも期待されている。


#### VALの提案

Visiolinguistic Attention Learning (VAL)を提案している。簡潔に説明すると、VALはCNNの中に複数の複合トランスフォーマを多段階で接続し、視覚特徴と言語意味論を合成することを特徴としている。我々のVALを訓練するために、我々は階層的なマッチングの目的を考案し、それは識別的特徴学習のために所望の視覚的特徴と意味的特徴への排他的な整列をインセンティブを与える。   VALの特徴は、アテンション学習を介して多レベルの視覚的特徴と言語的意味を合成する多重合成変換を行うことである。階層的なマッチングを目的とすることで、効果的な画像検索のために視覚的内容と言語的内容を合成表現でカプセル化することができる。  

テキストは、何らの分散表現
画像は、各レイヤーの特徴 (global pooling で集約している？)

Composite transformer 画像とテキストを共同で表現する部分

Fc is an MLP

 query, key, value (i.e.Q,K,V)　： self-attention


Fo

## 損失 (目標)
 我々の最終的な目的は、合成出力Foとターゲット画像表現Fの整合を図ることにあるので、我々は、次に詳述するように、希望する視覚的特徴と意味的特徴（図3）と整合するために、2つの損失を2つのレベルの階層で形成した階層的な整合目標を作成します（図3）。

 何やりたいか、
 Fo(参照画像 + テキスト) == ターゲット画像の特徴にしたい

  加えて、x_i, v (Wvsによりvisual spaceからsemantic spaceに射影されたもの)
  と 意味付けされたタグの差