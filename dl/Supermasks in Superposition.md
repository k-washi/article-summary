# [Supermasks in Superposition](https://arxiv.org/abs/2006.14769)


## abst

我々は、何千ものタスクを、壊滅的な忘却なしに連続的に学習することが可能なスーパーマスク・イン・スーパーポジショニング(SupSupSup)モデルを提示する。我々のアプローチでは、ランダムに初期化された固定ベースのネットワークを使用し、各タスクに対して良いパフォーマンスを達成するサブネットワーク(スーパーマスク)を見つけます。テスト時にタスクのアイデンティティが与えられれば、最小のメモリ使用量で正しいサブネットワークを検索することができます。もし与えられていなければ、SupSupは勾配ベースの最適化を使ってタスクを推論し、出力エントロピーを最小にする学習されたスーパーマスクの線形重ね合わせを見つけることができます。実際には、2500個のタスクの中から正しいマスクを識別するためには、1つの勾配ステップで十分であることがよくわかります。また、2つの有望な拡張機能も紹介します。まず、SupSupモデルは、新しいデータが不確かな場合に検出し、新しい学習分布に追加のスーパーマスクを割り当てることができるので、タスクの識別情報がなくても完全に学習することができます。最後に、成長するスーパーマスクの全体セットは、固定サイズのHopfieldネットワークのアトラクターとして暗黙的に格納することで、一定サイズのリザーバーに格納することができます。

## intor

多くの異なるタスクを忘却せず連続的に学習するこてゃ、ニューラルネットワークに取って重要な課題である。
例えば、ニューラルネットワークの重みを新しいタスクで訓練すると、以前のタスクのパフォーマンスが大幅に低下することがよくあり、これは破局的忘却として知られている問題です。

論文では、ネットワークの重みが固定でランダムなままであれば、破局的な忘却は起こらないという観測から始める。これを利用して、何千ものタスクを学習できる柔軟なモデル; Supermasks in Superposition (SupSup)を開発する。

SupSupのコアアイデア

a. 非訓練、ランダムに重み付けされたサブネットワークの表現力
ニューラルネットワークは、各接続を選択的に保持または除去し、サブネットワークを生成するバイナリマスクを重ねてもよい。可能なサブネットワークの数は、パラメータの数だけ組み合わせ可能である。研究者らは、ランダムに重み付けされたニューラルネットワーク内でさえ、複雑なタスクで良好な性能を達成する対応するサブネットワークを生成するスーパーマスクが存在するように、組み合わせの数が十分に大きいことを観察してきた。
ZhouやRamanujan et al.は、基礎となるネットワークの重みを固定かつランダムに保ちながら、これらのスーパーマスクを見つけるための2つのアルゴリズムを提示している。SupSupは、共有された非訓練ネットワークの上にあるスーパーマスクを各タスクごとに見つけることで、多くのタスクにスケーリングします。

b. 勾配ベースの最適化問題としてタスクの同一性の推論

タスクの識別が不明な場合、SupSupはタスクの識別を推測して正しいスーパーマスクを選択することができます。
タスクjからのデータが与えられたときの、訓練されたスーパーマスクを復元して使用することを目的としています。
このスーパーマスクは，タスクjからのデータが与えられたときに，自信を持った（すなわち，エントロピーの低い）出力分布を示すはず。

hendrycks 2016baselineでは、出力分布のエントロピーを最小化する学習済みスーパーマスクの凸型の組み合わせを求める最適化問題として、タスクの同一性の推論をフレーム化しています。

### Related work

以下を満たしていないものがほとんど。

1）タスクの同一性が訓練中に提供されるかどうか、2）推論中に提供されるかどうか、3）評価中にクラスラベルが共有されるかどうか、4）全体的なタスク空間が離散的であるか連続的であるかどうか。

例えば、タスクの同一性が訓練で提供されない場合、推論で提供することはもはや役に立たない。そのために、表1に示すように、適用可能な4つのシナリオをハイライトし、それぞれが適用可能な場合には離散的か連続的かをさらに分解している。

3つの最も重要なシナリオのバリエーションを明示的に扱う3文字の分類法によって、継続的な学習シナリオを分解している。
GN: TF
最初の2文字は、学習中, 推論中にタスクの同一性が与えられるかどうかを指定する。
三つ目の文字は、ラベルが共有されているかどうか（s｜u）。
モデルは正しいタスクIDとそのタスク内の正しいクラスの両方を予測しなければならない。共有の場合，モデルはタスク間で正しい共有ラベルを予測するだけでよいので，どのタスクからのデータかを表現したり，予測したりする必要はない。


この分類法を用いて、継続学習のための3つの既存のアプローチをレビューする。(Table.1)

(1) 正則化に基づく手法 Elastic Weight Consolidation (EWC) kirkpatrick2017overcoming や Synaptic Intelligence (SI) zenke2017continual のような手法は、壊滅的な忘却を軽減するために、前のタスクを解くために重要なパラメータの動きにペナルティを与える。パラメータの重要性の尺度は様々である；例えばEWCはフィッシャー情報行列pascanu2013revisitingを使用している。これらの方法は、以下のように動作します。G N s シナリオ（表1）。正則化のアプローチは、壊滅的な忘却を改善するが、正確には完全には解消されない。

(2) 実例、再生、生成モデルの利用 これらの手法は、明示的または暗黙的に（生成モデルを用いて）以前のタスクからデータを取得することを目的としています。例えば、rebuffi2017icarlは、特徴空間の中で最も近い模範に基づいて分類を実行します。さらに、lopez2017gradient; chaudhry2018efficientは、過去のタスクからの例に対してモデルの損失が増加するのを防ぐ一方で、rolnick2019experienceとshin2017continualは、それぞれ過去のデータを再生するためにメモリバッファと生成モデルを使用しています。データセット全体を正確に再生することで、壊滅的な忘却を些細なことでなくすことができますが、時間とメモリのコストがかかります。生成的アプローチは壊滅的な忘却を減らすことができるが、生成モデルは忘却の影響を受けやすい。

(3) タスク固有のモデル成分 学習目的を変更したり、データを再生したりする代わりに、様々な方法がある rusu2016progressive; yoon2017lifelong; mallya2018packnet; mallya2018piggyback; masse2018alleviating; xu2018reinforced; cheung2019superposition; golkar2019continual; wen2020batchensembleでは、タスクごとに異なるモデル成分を使用しています。進歩的ニューラルネットワーク（PNN）、動的拡張可能ネットワーク（DEN）、および強化継続学習（RCL）rusu2016progressive; yoon2017lifelong; xu2018reinforcedでは、モデルは新しいタスクごとに拡張されます。より効率的には、masse2018alleviatingはネットワークサイズを固定し、与えられたタスクに対してどのノードがアクティブであるかをランダムに割り当てます。mallya2018packnet; golkar2019continualでは、新しいタスクごとに不連続部分ネットワークの重みを学習します。サブネットワークの重みを学習する代わりに、新しいタスクごとに、Mallyaらmallya2018piggybackは、ImageNet上で事前に訓練されたネットワークに適用されるバイナリマスクを学習します。最近では、Cheungらcheung2019superpositionは、タスクごとに異なる（そしてほぼ直交する）コンテキストを使用することで、多くのモデルを1つに重ね合わせています。そして、正しいタスクコンテキストを用いて、タスクパラメータを効果的に取り出すことができる。最後に、BatchE wen2020batchensembleは、最初のタスクで共有重み行列を学習し、それ以降の各タスクではランク1の要素ごとのスケーリング行列のみを学習します。

https://rightcode.co.jp/blog/information-technology/machine-learning-catastrophic-forgetting

## 今回の手法は
我々の手法は、タスク固有のスーパーマスクを導入するため、この最終的なアプローチ(3)に該当します。しかし、このカテゴリの他の手法がすべてGGシナリオに限定されているのに対し、SupSupは4つのシナリオすべてで説得力のある性能を達成するために使用することができます。

## METHODS

このセクションでは、SupSup がスーパーマスクを利用して何千もの連続したタスクを忘れずに学習する方法を詳しく説明します。タスクの識別が与えられる簡単な設定から始め、タスクの識別ができないより難しいシナリオへと徐々に移行していきます。

## 
一般的な判別タスク。
x => dist p (1 ... l), p = f(x, W)
タスク(k個)で、入力サイズと分類ラベルlは、共有。

バイナリマスク(super mask)Mは、ランダムに重み付けされたNNに適用することができ、良好なサブネットワークに適用することができる。
super mask p = f(x, W*M) 

## 推論時を考える

推論時は、タスクjがわからない。 => タスクの特定が必要
推論時、タスク特定推論を目指す。これは、タスクjが所属するデータを特定することと同じで、そして、一致するsupermaskMjを選択する必要がある。

タスクID推論のための手順は以下の通りである。
1. (2)で表現
2. 出力のエントロピーが最小となるようにaを更新。
3. (4)でタスク推論

https://github.com/RAIVNLab/supsup/blob/master/mnist.ipynb